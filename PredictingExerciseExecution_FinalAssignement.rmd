---
title: "Predicting how well an exercise is done"
author: "Jorge Sauma"
date: "January 18, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Many people regularly quantify how much of a particular activity they do using devices such as Jawbone Up, Nike FuelBand, and Fitbit, but they rarely quantify how well they do it.
In this paper, we are going to use the data generated by those devices to create a prediction model that can tell if an exercise was done correctly, based on measurments from accelerometers on belts, forearms, arms, and dumbells.

## Loading Data
Data is provided by the HAR (Human Activity Resource) team. More info here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

Data is retrieved from the following locations:
```{r echo=FALSE}
setwd("/home/jsauma/Development/Courses/Data Science/Course 8 - Practical Machine Learning")
```

```{r}
training_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
training_set_raw <- read.csv(training_url)

test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
test_set_raw <- read.csv(test_url)
```

## Data inspection

A quick look at the columns, using str() (ouput not shown because it is lengthy), shows that there are several columns that are not relevant to the model, and even could produce a bad prediction. For example, if we keep the name of the subject (i.e. "Carlitos") in the model, it could be possible that the prediction is based on a name, which is wrong.

Therefore, the columns "X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", and "num_window", will be removed.

Besides this, we are going to remove the rows with NAs, because they could affect the model.

```{r}
training_set<-training_set_raw[ ,-(1:7)]
test_set<-test_set_raw[ ,-(1:7)]

#Remove NAs

training_set<-training_set[complete.cases(training_set), ]
test_set<-test_set[complete.cases(test_set), ]
```

Data is ready for the model.

## Model

According with the course material, Random Forests seems to be one of the best models for this kind of data, because accuracy is important. It will be used for our first modelling.

First, let's create the training set and validation set. The testing set will be the one stored in the test_set varaible. The data partition will be bassed on the "classe" column, and we will use 75% of the data for training and 25% for validation.

```{r}
library(caret)

training_partition <- createDataPartition(y = training_set$classe, p = 0.75, list=FALSE)
training <- training_set[training_partition,]
validation <- training_set[-training_partition,]
```

Model calculation:

```{r}
set.seed(2402)
modFit<-train(classe ~ ., data=training, method = "rf", proxy=TRUE)
```

The accuracy of the model is: 
```{r}
modFit$results
```

The accuracy of this model is around 75%. This might not be a good accuracy, so we need to modify the modelling process.

Our first thought was to use the RandomForest library instead of the caret library, but we got an error saying that the RandomForest function can't manage more than 53 variables.

This error indicates that this data set has too many predictors. So, the first task in order to improve accuracy is to reduce the number of variables. There are several ways to do this:

a) remove the columns which has NAs values 
b) remove the columns with small variance
c) focus on subsets of columns

We are going to try the first two options. However, since we removed the rows with NAs (a bad decision), we need to start over with the initial data set.

First, let's remove columns not needed, and the columns with NAs (order is important):

```{r}
training_set<-training_set_raw[ ,-(1:7)]
test_set<-test_set_raw[ ,-(1:7)]

#NAs
training_set<-training_set[,colSums(is.na(training_set)) == 0]
test_set <-test_set[,colSums(is.na(test_set)) == 0]

```

Next, in order to remove the predictor with near zero variance we are going to use the nearZeroVar function for the training set:

```{r}
nzv_columns <- nearZeroVar(training_set)
training_set <- training_set[-nzv_columns]
```

Let's check the new number of variables:

```{r}
dim(training_set)
```

There are  53 variables now. We can proceed.

Now, let's try again with the model:

```{r}
training_partition_var <- createDataPartition(y = training_set$classe, p = 0.75, list=FALSE)
training_nzv <- training_set[training_partition_var,]
validation_nzv <- training_set[-training_partition_var,]
modFit_nzv<-train(classe ~ ., data=training_nzv, method = "rf", proxy=TRUE)
```

The accuracy of the new model is: 
```{r}
modFit_nzv$results
```

Accuracy is around 99%. This model, with the new data set, is suitable for predicting the classe value.

The low accuracy of the first model was produced by the wrong decisions during the exploratory data analysis. The initial decision of removing the rows with NA was not the right one.

##Test data prediction

The predicted values for the test set are:

```{r}
predict(modFit_nzv, test_set)
```

